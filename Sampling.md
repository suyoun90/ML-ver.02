## 1. Under Sampling 

#### ■ 장점
다수의 클래스 데이터를 제거하므로 계산시간이 감소되고 클래스 오버랩을 줄일 수 있음   
  
#### ■ 단점
데이터를 제거하므로 정보 손실이 발생
  
#### ■ 종류 
  
**1) Random Under Sampling (RUS)**
-  다수의 클래스에 속해있는 관측치들 중 무작위로 샘플링 
-  수행할 때마다 다른 결과가 도출됨 
-  샘플링 마다 모델 성능이 달라지긴 하지만 의외로 괜찮게 사용되는 경우가 많음 
　　  
**2) Tomek Links (토멕링크)**  
![image](https://user-images.githubusercontent.com/63949445/108614759-c6fb0280-7440-11eb-8d61-f8e8f68efa06.png)  
- Tomek links는 서로 다른 클래스에 속하는 항 쌍의 데이터를 뜻함  
- 가까운 한 쌍의 데이터를 찾은 다음 그 중에서 다수 클래스(majority class)에 속하는 데이터를 제거하는 방법
- 다수 클래스의 데이터를 제거함으로써 데이터 불균형 문제도 해결 되는 동시에 두 클래스간의 거리가 멀어지므로 분류 문제를 조금 더 수월하게 만들어줌  
- 여전히 데이터 손실에 대한 문제를 갖고 있으므로 유의해야 함 
　  
**3) CNN (Condensed Nearest Neighbor)** 
- 소수 클래스에 속하는 데이터 전체 A + 다수 클래스에 속하는 데이터 중 무작위로 선택한 하나의 데이터 B 로 구성된 Sub-데이터를 구성  
- 다수 클래스에 속한 나머지 데이터들 중 하나씩 KNN에서 K=1인 1-Nearest Neighbors를 이용해 그 데이터가 무작위로 선택한 다수 클래스 데이터 한개 (B)와 가까운지, 소수 클래스(A) 중 어떤 것이든 그것과 가까운지를 판정
- 다수 클래스에 속하는 나머지 데이터들 중 소수 클래스(A) 데이터와 더 가까운 데이터를 소수 클래스로 우선 분류 
- 이후 1-Nearest Neighbors를 통해 정상적으로 분류된 다수 클래스 데이터를 언더 샘플링
- 이 때 K는 반드시 1이어야 함 (자세한 설명은 김성범 교수님 강의 참고)
　  
**4) OSS(One-side Selection)**
- Tomel links와 CNN을 같이 수행하는 방식  
- Tomel links로 분류 경계에 존재하는 데이터들을 언더샘플링 하는 동시에 CNN으로 분류가 잘 되는 데이터를 언더샘플링 할 수 있음  

  
　  
　  
## 2. Over Sampling  

#### ■ 장점
데이터 정보 손실이 없고 언더샘플링에 비해 높은 분류 정확도를 보임  

#### ■ 단점
과적합 발생 가능, 계산 시간 증가, 노이즈 또는 이상치에 민감함  

#### ■ 종류 

**1) Resampling**
-  소수 클래스에 속하는 데이터의 관측치를 복사하는 방식
-  기존 데이터와 동일하게 copy하므로 소수 클래스에 과적합 발생 가능  
　  
**2) SMOTE**   
![image](https://user-images.githubusercontent.com/63949445/108615458-9ec2d200-7447-11eb-8da6-365319376e38.png)  
- 소수 클래스에 속하는 데이터 주변에 원본 데이터와 동일하지 않으면서 소수 클래스에 해당하는 가상의 데이터를 생성 
- K값을 사전에 정한 후 임의의 데이터 하나를 선정하고 이 데이터와 가장 가까운 상위 k개의 데이터 중 하나를 랜덤으로 선정해 가상의 데이터를 계산 
- K는 2 이상의 정수값으로 설정해야 함. K가 1인 경우 데이터를 고를 때 선택지가 1개밖에 없기 때문   
　  
**3) Borderline - SMOTE**   
- 두 클래스 데이터간의 경계선 부분에만 SMOTE 적용  
- 보더라인을 찾는 방법은 소수 클래스에 속하는 데이터 하나를 선정해 주변의 N개를 탐색한 후 이 N개 중 다수 클래스에 속하는 데이터가 몇 개인지 확인함. 이 때 다수 클래스에 속하는 개수 K에 따라 보더라인인지의 여부를 결정함  
- 계산 공식은 특정 논문에 소개 되어 있음. 이러한 방법이 있다 정도만 일단 참고   
　  
**4) ADASYN**  
- Adaptive Synthetic Sampling Approach로, Borderline SMOTE와 비슷하지만 샘플링 개수를 데이터 위치에 따라 다르게 설정한다는 것이 차이점임  
- 먼저 모든 소수 클래스 데이터 각각에 대해 주변 데이터 K개를 탐색하고 그 중 다수 클래스 관측치의 비율을 계산   
- 이 값에 대해 (다수 클래스 개수 - 소수 클래스 개수)를 곱해주고 반올림을 한 정수값 만큼 각 데이터에서 SMOTE 방식을 이용해 오버샘플링 수행  
- 소수 클래스 주변의 다수 클래스 개수에 따라 유동적으로 오버샘플링 개수를 생성  
- 데이터의 분포도를 그렸을 때 다수 클래스가 여러 부분에 분포되어 있을 때 ADASYN이 잘 동작   
　
>참고 : https://techblog-history-younghunjo1.tistory.com/123
